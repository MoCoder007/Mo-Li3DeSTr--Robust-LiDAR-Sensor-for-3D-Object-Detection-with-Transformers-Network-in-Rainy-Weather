from scipy.spatial import KDTree

LABEL_TO_INDEX = {'Car': 0,'Cyclist': 1}
INDEX_TO_LABEL = {0: 'Car', 1: 'Cyclist'}

# Reads annotations from a file, converting bounding box information to a format suitable for processing
def read_annotations(file_path):
    annotations = []
    with open(file_path, 'r') as file:
        for line in file:
            parts = line.strip().split(' ')
            class_label = parts[0]
            bbox_center = np.array([float(parts[i]) for i in range(1, 4)])
            bbox_dims = np.array([float(parts[i]) for i in range(4, 7)])
            bbox = np.concatenate([bbox_center - bbox_dims / 2, bbox_center + bbox_dims / 2])  # Convert to [x_min, y_min, z_min, x_max, y_max, z_max]
            annotations.append({
                'class_label': LABEL_TO_INDEX.get(class_label, -1),
                'bbox': bbox
            })
    return annotations

# Filters noise from LiDAR data based on the density of points in the vicinity, keeping points in denser areas.
def filter_noise(lidar_data, k=20, x_threshold=2.0):
    tree = KDTree(lidar_data[:, :3])
    distances, _ = tree.query(lidar_data[:, :3], k=k+1)
    distances = distances[:, 1:]  # Exclude self-match
    mean_distances = np.mean(distances, axis=1)
    std_dev = np.std(mean_distances)
    mean = np.mean(mean_distances)
    filtered_indices = mean_distances < (mean + x_threshold * std_dev)
    return lidar_data[filtered_indices], filtered_indices

# Defines a dataset for loading and preprocessing LiDAR data and annotations for the KITTI dataset.
class KITTIDataset(Dataset):
    def __init__(self, lidar_root_dir, annotation_root_dir, split='train', max_points=120000):
        self.lidar_root_dir = lidar_root_dir
        self.annotation_root_dir = annotation_root_dir
        self.split = split
        self.max_points = max_points
        self.lidar_files = self.get_files(lidar_root_dir, 'velodyne')
        self.annotation_files = self.get_files(annotation_root_dir, 'label_2')

    # Retrieves file paths for the dataset, supporting both LiDAR and annotation data.
    def get_files(self, root_dir, subdir):
        files = []
        path = os.path.join(root_dir, self.split, subdir)
        for file_name in sorted(os.listdir(path)):
            if file_name.endswith('.bin') or file_name.endswith('.txt'):
                files.append(os.path.join(path, file_name))
        return files

    # Returns the number of items in the dataset.
    def __len__(self):
        return len(self.lidar_files)

    # Loads and preprocesses the data for a given index, including noise filtering and label assignment.
    def __getitem__(self, idx):
        preprocessed_data_path = os.path.join(self.lidar_root_dir, f'preprocessed_data_{idx}.npy')
        preprocessed_labels_path = os.path.join(self.lidar_root_dir, f'preprocessed_labels_{idx}.npy')
        
        if os.path.exists(preprocessed_data_path) and os.path.exists(preprocessed_labels_path):
            lidar_data = np.load(preprocessed_data_path)
            class_labels = np.load(preprocessed_labels_path)
        else:
            lidar_data = np.fromfile(self.lidar_files[idx], dtype=np.float32).reshape(-1, 4)
            lidar_data = self.preprocess_lidar_data(lidar_data)
            annotations = read_annotations(self.annotation_files[idx])
            filtered_lidar_data, _ = filter_noise(lidar_data)
            class_labels = self.assign_labels(filtered_lidar_data, annotations)
            np.save(preprocessed_data_path, filtered_lidar_data)
            np.save(preprocessed_labels_path, class_labels)
            lidar_data = filtered_lidar_data
        
        padded_lidar_data, padded_class_labels = self.pad_or_truncate(lidar_data, class_labels)
        return torch.tensor(padded_lidar_data, dtype=torch.float32), torch.tensor(padded_class_labels, dtype=torch.long)

    # Filters LiDAR points based on altitude and distance to keep points within a certain range.
    def preprocess_lidar_data(self, lidar_data):
        z_min, z_max = -1.5, 2.0
        distance_max = 50.0
        distances = np.sqrt(np.sum(lidar_data[:, :3]**2, axis=1))
        altitude_filter = (lidar_data[:, 2] >= z_min) & (lidar_data[:, 2] <= z_max)
        distance_filter = distances <= distance_max
        filtered_indices = altitude_filter & distance_filter
        return lidar_data[filtered_indices]

    # Pads or truncates the LiDAR data and class labels to a fixed size for consistent input size.
    def pad_or_truncate(self, lidar_data, class_labels):
        num_points = lidar_data.shape[0]
        if num_points < self.max_points:
            padding = np.zeros((self.max_points - num_points, lidar_data.shape[1]), dtype=np.float32)
            label_padding = -np.ones(self.max_points - num_points, dtype=np.int64)
            lidar_data = np.vstack((lidar_data, padding))
            class_labels = np.concatenate((class_labels, label_padding))
        elif num_points > self.max_points:
            lidar_data = lidar_data[:self.max_points, :]
            class_labels = class_labels[:self.max_points]
        return lidar_data, class_labels

    # Assigns class labels to LiDAR points based on their location relative to annotated bounding boxes.
    def assign_labels(self, lidar_data, annotations):
        class_labels = np.full(lidar_data.shape[0], -1, dtype=np.int64)
        for annotation in annotations:
            bbox = annotation['bbox']
            in_bbox_indices = np.where(
                (lidar_data[:, 0] >= bbox[0]) & (lidar_data[:, 0] <= bbox[3]) &
                (lidar_data[:, 1] >= bbox[1]) & (lidar_data[:, 1] <= bbox[4]) &
                (lidar_data[:, 2] >= bbox[2]) & (lidar_data[:, 2] <= bbox[5])
            )[0]
            class_labels[in_bbox_indices] = annotation['class_label']
        return class_labels

# Specify your dataset directories
lidar_root_dir = 'D:/data_object_velodyne'
annotation_root_dir = 'D:/data_object_label_2'
kitti_dataset = KITTIDataset(lidar_root_dir, annotation_root_dir)

data_loader = DataLoader(kitti_dataset, batch_size=1, shuffle=True)

# Initialize an empty list to store processed samples
processed_samples = []

# Iterate over the DataLoader

for batch_idx, (data, labels) in enumerate(data_loader):
    labels_flattened = labels.view(-1)
    string_labels = [INDEX_TO_LABEL[label.item()] if label.item() in INDEX_TO_LABEL else 'Unknown' for label in labels_flattened]
    
    for sample_idx in range(data.size(0)):
        if string_labels[sample_idx] != 'Unknown':  # Check if class label is not 'Unknown'
            print(f"Batch {batch_idx}: Point of Samples: {data[sample_idx, 1].cpu()}, Class Label: {string_labels[sample_idx]}")

    processed_samples.append((data, labels))
    
    if batch_idx == 6000:  
        break
