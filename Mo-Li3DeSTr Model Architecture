#  Mo-Li3DeSTr Transformer model's architecture

import torch
import torch.nn as nn
import torch.nn.functional as F

# Defines the main transformer model architecture specifically designed for processing LiDAR data
# for object detection tasks. Incorporates a robust embedding layer, positional encoding, 
# and a sequence of transformer encoder layers to encode the input point cloud data.
class LiDARTransformer(nn.Module):
    def __init__(self, num_classes, dim_model, num_heads, num_encoder_layers, hidden_dim, num_proposals):
        super(LiDARTransformer, self).__init__()
        # Embedding layer to project input LiDAR point clouds into a high-dimensional space
        self.embedding = RobustPointNetEmbeddingLayer()
        # Adds positional information to the embedded points for retaining structural information
        self.positional_encoding = PositionalEncoding3D(dim_model)
        # Module to model and mitigate the effects of rain on LiDAR data
        self.rain_effect_module = RainEffectModule()

        # Transformer encoder layers for processing the positional-encoded embeddings
        self.encoder_layers = nn.ModuleList([
            TransformerEncoderLayer(dim_model, num_heads, hidden_dim)  # Corrected initialization
            for _ in range(num_encoder_layers)
        ])
         # Head for detecting objects within the processed point cloud
        self.object_detection_head = ObjectDetectionHead(dim_model, num_classes, num_proposals)
       
   
    def forward(self, x):
        # Initial processing of LiDAR data to mitigate rain effects
        x = x.transpose(0, 2)
        x = self.rain_effect_module(x)
        
        # Embedding and positional encoding
        x = self.embedding(x)
        x = self.positional_encoding(x)
        
        # Pass through encoder layers
        for layer in self.encoder_layers:
            x = layer(x)
        print(detection_output)
        
        # Object detection
        detection_output = self.object_detection_head(x)
        return detection_output
   

# Adds 3D positional encoding to input embeddings to retain the geometric structure of the point cloud.
class PositionalEncoding3D(nn.Module):
    """ Positional Encoding for 3D point clouds """
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding3D, self).__init__()
        self.d_model = d_model

        # Create constant 'pe' matrix with values dependent on 
        # pos and i (position and dimension)
        pe = torch.zeros(max_len, d_model)
        for pos in range(max_len):
            for i in range(0, d_model, 3):
                if i + 2 < d_model:
                    pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))
                    pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))
                    pe[pos, i + 2] = math.sin(pos / (10000 ** ((2 * (i + 2))/d_model)))

        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        # Make embeddings relatively larger
        x = x * math.sqrt(self.d_model)
        # Add constant to embedding
        seq_len = x.size(1)
        pe = self.pe[:, :seq_len]
        x = x + pe
        return x

# Optionally downsamples or applies an enhanced filtering to the input point cloud to reduce
# computational load or improve model focus on relevant features.
def enhanced_filtering_or_downsampling(x):
    # Ensure x has at least three dimensions before attempting to index it as such
    if x.dim() == 3:
        # Perform downsampling; assuming x's shape is [batch_size, num_features, num_points]
         return x[:, :, ::2]  # Example: taking every second point for simplicity
    else:
        # Handle unexpected tensor shapes, perhaps by logging or raising an error
        print(f"Unexpected tensor shape: {x.shape}")
        return x  # Or handle appropriately


# A mini-network that predicts an affine transformation matrix to achieve spatial invariance.
class TNet(nn.Module):
    """ T-Net mini network in PointNet """
    def __init__(self, k=4):
        super(TNet, self).__init__()
        self.k = k
        self.conv1 = nn.Conv1d(4, 64, 1)
        self.conv2 = nn.Conv1d(64, 128, 1)
        self.conv3 = nn.Conv1d(128, 1024, 1)
        self.fc1 = nn.Linear(1024, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, k*k)

        self.bn1 = nn.BatchNorm1d(64)
        self.bn2 = nn.BatchNorm1d(128)
        self.bn3 = nn.BatchNorm1d(1024)
        self.bn4 = nn.BatchNorm1d(512)
        self.bn5 = nn.BatchNorm1d(256)

        self.fc3.bias.data.fill_(0)
        self.fc3.weight.data.fill_(0)

    def forward(self, x):
        batch_size = x.size()[0]

        x = F.relu(self.bn1(self.conv1(x)))
        x = F.relu(self.bn2(self.conv2(x)))
        x = F.relu(self.bn3(self.conv3(x)))

        x = nn.MaxPool1d(x.size(-1))(x)
        x = x.view(batch_size, -1)
        x = F.relu(self.bn4(self.fc1(x)))
        x = F.relu(self.bn5(self.fc2(x)))
        x = self.fc3(x)

        iden = torch.eye(self.k).view(1, self.k*self.k).repeat(batch_size, 1)
        if x.is_cuda:
            iden = iden.cuda()
        x = x.view(batch_size, self.k, self.k) + iden
        return x

# Extracts robust features from input point clouds by incorporating a TNet for spatial
# invariance and a series of convolutions for feature extraction.
class RobustPointNetEmbeddingLayer(nn.Module):
    """ Modified PointNet-based feature extraction for robustness """
    def __init__(self):
        super(RobustPointNetEmbeddingLayer, self).__init__()
        self.tnet = TNet(k=4)

        # PointNet embedding layers
        self.conv1 = nn.Conv1d(4, 64, 1)
        self.conv2 = nn.Conv1d(64, 128, 1)
        self.conv3 = nn.Conv1d(128, 1024, 1)
        self.bn1 = nn.BatchNorm1d(64)
        self.bn2 = nn.BatchNorm1d(128)
        self.bn3 = nn.BatchNorm1d(1024)

    def forward(self, x):
        # Apply enhanced filtering or downsampling
        x = x.transpose(1, 1)
        x = enhanced_filtering_or_downsampling(x)

        # Assuming x is [batch_size, 4, num_points] and you want to exclude the intensity channel
        print("Shape of x before slicing:", x.shape)
        x = x[:, :3, :]  # Keep only the first 3 channels (x, y, z) for TNet
       
        # Transform the input
        trans = self.tnet(x)
        x = x.transpose(1, 1)
        x = torch.bmm(x, trans)
        x = x.transpose(1, 1)

        # Apply PointNet embedding layers
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.relu(self.bn2(self.conv2(x)))
        x = self.bn3(self.conv3(x))

        # Max pooling over the points
        x = nn.MaxPool1d(x.size(-1))(x)
        x = x.view(-1, 1024)
        return x

# Estimates the likelihood of noise in the input data to adjust the attention mechanism
# accordingly, aiming to reduce the impact of noisy data on model predictions.
class NoiseEstimationNetwork(nn.Module):
    """Network to estimate the noise likelihood of input points."""
    def __init__(self, input_dim, hidden_dim):
        super(NoiseEstimationNetwork, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        noise_weight = torch.sigmoid(self.fc2(x))  # Output between 0 and 1
        return noise_weight

# Custom implementation of a Multi-Head Attention mechanism that integrates noise weighting
# to adaptively focus on less noisy data, potentially improving model performance in adverse conditions.        
class MultiHeadAttention(nn.Module):
    """ Enhanced Multi-Head Attention mechanism with noise-aware weighting """
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        assert self.head_dim * num_heads == d_model, "d_model must be divisible by num_heads"

        self.linear_keys = nn.Linear(d_model, d_model)
        self.linear_values = nn.Linear(d_model, d_model)
        self.linear_queries = nn.Linear(d_model, d_model)
        self.fc_out = nn.Linear(d_model, d_model)

        # Noise Estimation Network
        self.noise_net = NoiseEstimationNetwork(input_dim=d_model, hidden_dim=128)

    def forward(self, keys, values, queries, mask):
        batch_size = queries.size(0)

        # Linear transformations, split into num_heads
        keys = self.linear_keys(keys).view(batch_size, -1, self.num_heads, self.head_dim)
        values = self.linear_values(values).view(batch_size, -1, self.num_heads, self.head_dim)
        queries = self.linear_queries(queries).view(batch_size, -1, self.num_heads, self.head_dim)

        # Scaled Dot-Product Attention
        attention = torch.einsum("bnqd,bnkd->bnqk", [queries, keys])
        if mask is not None:
            attention = attention.masked_fill(mask == 0, float("-1e20"))

        # Compute noise weights
        noise_weights = self.noise_net(queries).view(batch_size, -1, 1, 1)
        attention = attention * noise_weights  # Adjust attention with noise weights

        attention = F.softmax(attention / (self.d_model ** 0.5), dim=3)

        # Apply attention to values
        out = torch.einsum("bnqk,bnvd->bnqd", [attention, values]).reshape(batch_size, -1, self.d_model)

        # Final linear transformation
        out = self.fc_out(out)
        return out

# Defines a feed-forward network used within the transformer encoder layers, 
# providing additional processing capacity to the model.
class FeedForward(nn.Module):
    """ Feed Forward network """
    def __init__(self, d_model, hidden_dim):
        super(FeedForward, self).__init__()
        self.linear1 = nn.Linear(d_model, hidden_dim)
        self.linear2 = nn.Linear(hidden_dim, d_model)

    def forward(self, x):
        x = F.relu(self.linear1(x))
        x = self.linear2(x)
        return x

# A single layer of the transformer encoder, combining multi-head attention and a feed-forward
# network to process and enhance the input feature representation.
class TransformerEncoderLayer(nn.Module):
    """ A single Transformer Encoder Layer """
    def __init__(self, d_model, num_heads, hidden_dim):
        super(TransformerEncoderLayer, self).__init__()
        self.attention = MultiHeadAttention(d_model, num_heads)
        self.norm1 = nn.LayerNorm(d_model)
        self.ff = FeedForward(d_model, hidden_dim)
        self.norm2 = nn.LayerNorm(d_model)

    def forward(self, x, mask):
        # Multi-Head Attention
        attention = self.attention(x, x, x, mask)
        x = self.norm1(attention + x)

        # Feed Forward
        ff_out = self.ff(x)
        x = self.norm2(ff_out + x)
        return x

# Predicts object classes and bounding boxes from the encoded features produced by the transformer model,
# utilizing a series of convolutional layers for the final detection task.
class ObjectDetectionHead(nn.Module):
    """ Object Detection Head for final predictions """
    def __init__(self, d_model, num_classes, num_proposals):
        super(ObjectDetectionHead, self).__init__()
        self.d_model = d_model
        self.num_classes = num_classes
        self.num_proposals = num_proposals

        # Convert the Transformer's encoded features to a suitable format
        self.conv1 = nn.Conv1d(d_model, 256, 1)
        self.conv2 = nn.Conv1d(256, 128, 1)

        # Predict bounding boxes and class scores
        self.bbox_pred = nn.Conv1d(128, num_proposals * 6, 1)  # 6 = 3 for center coordinates + 3 for size
        self.class_score = nn.Conv1d(128, num_proposals * num_classes, 1)

    def forward(self, x):
        # Assuming x is of shape (batch_size, d_model, num_points)
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))

        # Predict bounding boxes and class scores
        bbox = self.bbox_pred(x)
        scores = self.class_score(x)

        # Reshape to (batch_size, num_proposals, -1)
        bbox = bbox.view(-1, self.num_proposals, 6)
        scores = scores.view(-1, self.num_proposals, self.num_classes)
        print(bbox)
        return bbox, scores

# Mitigates the effects of rain on LiDAR data, aiming to enhance the quality of
# the input data under adverse weather conditions.
class RainEffectModule(nn.Module):
    """ Module to identify rain-distorted inputs in LiDAR data. """
    def __init__(self):
        super(RainEffectModule, self).__init__()
        # Example architecture: A few convolutional layers followed by a global pooling
        self.conv1 = nn.Conv1d(4, 64, 1)  # Assuming 4 features (x, y, z, intensity)
        self.bn1 = nn.BatchNorm1d(64)
        self.conv2 = nn.Conv1d(64, 128, 4)
        self.bn2 = nn.BatchNorm1d(128)
        self.conv3 = nn.Conv1d(128, 256, 4)
        self.bn3 = nn.BatchNorm1d(256)
        self.fc1 = nn.Linear(256, 128)
        self.fc2 = nn.Linear(128, 4)  # Output a single value representing rain effect

    def forward(self, x):
        # Transpose input tensor to match expected shape [batch_size, 4, num_points]
        #x = x.permute(0, 2, 1)

        # Apply convolutional layers with ReLU and batch normalization
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.relu(self.bn2(self.conv2(x)))
        x = F.relu(self.bn3(self.conv3(x)))

        # Global max pooling
        x = nn.MaxPool1d(x.size(-1))(x)
        x = x.squeeze(dim=2)  # Remove the last dimension (num_points)
        
        # Fully connected layers
        x = F.relu(self.fc1(x))
        
        # Reshape tensor to [batch_size, -1] before passing to fc2
        x = x.view(x.size(0), -1)
        
        rain_effect = torch.sigmoid(self.fc2(x))  # Output a value between 0 and 1
        print(rain_effect)
        return rain_effect
